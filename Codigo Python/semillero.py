# -*- coding: utf-8 -*-
"""Semillero.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SQCB15uj2Pwbu-JfHq3szhyMiggMebBG

# Informe preparación de la data SAJR-1221
##### Integrantes: ***Jaime Lemus***

# Introducción:

La calidad del aire es uno de los enfoques más importantes para este momento de la humanidad. Según la onu 1/3 de las muertes en el mundo son gracias a fuentes de emisión las cuales se encuentran a nuestro alrededor, gases emitidos de autos, humo del tabaco, material particulado producido de operaciones industriales, etc. Por eso por medio de la recolección de datos de las estaciones meteorológicas, se plantea predecir el nivel de monóxido de carbono dentro de la universidad Libre. Los datos se pueden consultar en el siguieten Link http://rmcab.ambientebogota.gov.co/report/MonitorReport

# Integración de la data

En primer lugar, se toman los datos desde el 01-08-2021 hora 01:00 am hasta el 31-07-2022 hora 24:00 pm. En total 8761 registros, pero el formato que se nos entrega en la plataforma es xls, el formato de excel, además de ello se nos entrega con diferentes casillas llenas de imágenes y letras, los cuales no permitían que se leyera dentro del lenguaje, por lo cual se borraron estas manualmente sin alterar la data. Al mismo tiempo se convirtió la data de xls a csv para trabajarla de forma correcta, y se adiciona un "index=false" por si queda un registro como título o una letra el cual al momento de convertir pueda generar algún problema. Después de ello se revisó la data, entre estos registros tienen más valores nulos.
"""

#Importamos las librerias las cuales se trabajaran enlas diferentes partes de este trabajo
import numpy as np
import matplotlib.pyplot as plt 
import pandas as pd  
import seaborn as sns
from sklearn import svm
#De la libreria numpy exportamos las siguietens librerias
from numpy import mean, ptp, var, std, array, cov, corrcoef
#Instalamos la libreria pingouin para el analisis de correlaciones
!pip uninstall scipy -y
!pip uninstall pingouin -y
!pip install pingouin
import pingouin as pg

#Importamos la data de xls a csv
dfxls = pd.read_excel('StationsReport.xlsx')
dfxls.to_csv('StationsReport.csv', encoding='utf-8', index=False)
#Leemos la data
df = pd.read_csv('StationsReport.csv')

#Usaremos el nombre csv para trabajar el DataFrame
csvv = df

#Revisamos los registros
csvv.info()

#Revisamos el tamaño
csvv.shape

#Valores nulos dentro del data frame
print(csvv.isnull().sum())

data1 = csvv
fecha1 = csvv

"""Para este proyecto separamos totalmente la hora, ya que solo necesitaremos el día por lo cual se eliminan el mes, año y hora."""

#Separamos el dia de las fechas  
data1 = fecha1[  'DateTime'  ]. str  .split(pat =   '-'  , n =   1  , expand =   True  )  
data1 = data1.drop([  1  ], axis=  1  )  
fecha1[  'DateTime'  ] = data1
csvv

"""# Eliminación de variables irrelevantes y redundantes

Para este momento del proyecto, eliminamos las variables que más tienen datos nulos y aquellas las cuales no son necesarias para el análisis del proyecto, como el material particulado (PM) dióxido de carbono, radiación solar entre otras.
"""

# Eliminamos comlumnas
csvv=csvv.drop(['PM10 µg/m3'], axis=1)
csvv=csvv.drop(['SO2 ppb'], axis=1)
csvv=csvv.drop(['CO2 ppm'], axis=1)
csvv=csvv.drop(['PM2.5 µg/m3'], axis=1)
csvv=csvv.drop(['Precipitacion mm'], axis=1)
csvv=csvv.drop(['Rad Solar W/M²'], axis=1)

"""#Limpieza de datos

Buscamos datos nulos dentro del DF, esto para eliminar estos registros, de la misma forma cambiamos datos de un tipo objeto a tipo entero y cambiamos el nombre de las columnas para evitar errores en los siguientes pasos.
"""

#Eliminamos valores nulos
csvv = csvv[csvv['CO ppm'].notna()]
csvv = csvv[csvv['OZONO ppb'].notna()]
csvv = csvv[csvv['NO ppb'].notna()]
csvv = csvv[csvv['NO2 ppb'].notna()]
csvv = csvv[csvv['NOX ppb'].notna()]
csvv = csvv[csvv['Vel Viento m/s'].notna()]
csvv = csvv[csvv['Dir Viento Grados'].notna()]
csvv = csvv[csvv['Temperatura °C'].notna()]
csvv = csvv[csvv['HR %'].notna()]

#Opcional para borrar valores nullos
csvv = csvv.dropna(how='all')
csvv.shape

#Revision valores nulos dentro del data frame
print(csvv.isnull().sum())

csvv

#COnvertimos de objeto a entero la primera columna
csvv = csvv.astype({'DateTime':'int'})
csvv.info()

#cambiamos el nombre de las columnas para evitar errores
csvv.columns= (['DateTime', 'CO_ppm', 'OZONO_ppb', 'NO_ppb', 'NO2_ppb', 'NOX_ppb',
       'Vel_Viento_ms', 'Dir_Viento_Grados', 'Temperatura_C', 'HR'])

"""#Creación de nueva variables

Es probable que no se puedan colocar en esta sección ya que se necesitan para áreas muy específicas del código, por lo cual procederé a listar que hace cada una y por que fue creada. 
 
 
*   (csvv = df) Se uso para trabajar el data frame completamente la idea de este es preservar los datos de la forma mas integra posible
 
*   (data1 = csvv y fecha1 = csvv) Se crearon con el fin de tener un respaldo del data frame para separar las fechas y no afectar el df original
 
*   (data) Esta variable se trabajara en análisis de correlación para mostrar los datos del csvv sin tener que usar completamente todos los elementos del DataFrame original.

#Descripción estadística de los datos

Revisamos la varianza, desviación estándar y coeficiente de variación de las variables dentro del data frame, esto para revisar qué variables se podrían llegar a trabajar para la predicción del modelo.
"""

#Varianza
print('Varianza= ')
print(var(csvv))
print("--------------------------------------------")
#Desviación Estándar
print('Desv Estandar= ')
print(std(csvv))
#Coeficiente de variación
print("--------------------------------------------")
print('Coeficiente de variación')
print(mean(csvv) / std(csvv))
print("--------------------------------------------")

csvv.CO_ppm.hist()

csvv.NO_ppb.hist()

"""Se encuentra que tanto en el CO como en NO, el histograma está sesgado a la derecha, de manera no regular. Ya que parece que los datos se están concentrando más sobre 0 que sobre las demás partes regulares del histograma."""

csvv.NO2_ppb.hist()

csvv.plot.scatter(x='DateTime', y='CO_ppm')

csvv.plot.scatter(x='Vel_Viento_ms', y='CO_ppm')

csvv.plot.scatter(x='CO_ppm', y='NOX_ppb')

csvv.plot.scatter(x='CO_ppm', y='NO_ppb')

"""Al mismo tiempo se analizó diferentes variables las cuales por cómo se comportan los datos se podrían ver, y se encontraron 4 diagramas los cuales fueron seleccionados, que fue en qué días se encuentra más concentración de monóxido de carbono. Cómo afecta la velocidad del viento a el monóxido de carbono (encontrándose que entre menos viento más monóxido de carbono en la zona) se encontró una relación lineal entre las variables del óxido de nitrógeno y el óxido nítrico con respecto a el monóxido de carbono.

#Análisis de correlaciones

Usamos la descripción estadística de los datos para buscar correlaciones con la data que se trabaja, la primera fue sobre el tiempo. Aunque el primer análisis es bueno, cuando se halla el intervalo de confianza CI95%, se encuentra que no se puede confiar en la variable sobre el tiempo, pero se encuentra una excelente relación con respecto a óxido de nitrógeno. Para hallar este intervalo de confianza fue necesario instalar la librería pingouin (En el inicio esta), la cual solo añade algunas funciones para análisis estadístico.
"""

data = array([csvv.DateTime, csvv.CO_ppm])
cov(data, bias=1)
#Calcular el coeficiente de correlación de los dos grupos
corrcoef(data)

data = array([csvv.Vel_Viento_ms, csvv.CO_ppm])
cov(data, bias=1)
#Calcular el coeficiente de correlación de los dos grupos
corrcoef(data)

from scipy import stats
# Cálculo de correlación y significancia con Scipy
r, p = stats.pearsonr(csvv['DateTime'], csvv['CO_ppm'])
print(f"Correlación Pearson: r={r}, p-value={p}")
print("--------------------------------------------")
r, p = stats.spearmanr(csvv['DateTime'], csvv['CO_ppm'])
print(f"Correlación Spearman: r={r}, p-value={p}")
print("--------------------------------------------")
r, p = stats.kendalltau(csvv['DateTime'], csvv['CO_ppm'])
print(f"Correlación Pearson: r={r}, p-value={p}")

r, p = stats.pearsonr(csvv['NO_ppb'], csvv['CO_ppm'])
print(f"Correlación Pearson: r={r}, p-value={p}")
print("--------------------------------------------")
r, p = stats.spearmanr(csvv['NO_ppb'], csvv['CO_ppm'])
print(f"Correlación Spearman: r={r}, p-value={p}")
print("--------------------------------------------")
r, p = stats.kendalltau(csvv['NO_ppb'], csvv['CO_ppm'])
print(f"Correlación Pearson: r={r}, p-value={p}")

"""Para entender los resultados de las ejecuciones que se muetran a continuación, primero necesitamos entender qué significan las variables las cuales se están midiendo:
 
 
*   n= # de muestras dentro del Data Frame
 
*   r= El coeficiente de correlación sobre la intensidad de la relación lineal entre dos variables
 
*   CI95% = También llamado intervalo de confianza, es donde está concentrado la mayor cantidad de datos (95%), los datos que se encuentran en los extremos son valores atípicos, corresponden al (2,5%) en ambos extremos, dando como resultado un 5%
*   p-val = Es el 5% de los datos atípicos de los cuales se habló en el concepto anterior, esto es para demostrar las hipótesis dentro de las probabilidades. 
*   BF10 = Tambien llamado analisis o estadistica Bayesiana es para medir que tan compatible son las variables (Aunque para este no se nos muestra mayor información) 
*   Power = Incrementa junto con el tamaño de la muestra, por lo general se relaciona con el p-val. 





"""

import pingouin as pg
#Analisis mas profundo sobre las correlaciones
display(pg.corr(csvv['CO_ppm'], csvv['Vel_Viento_ms'], method='pearson'))
display(pg.corr(csvv['CO_ppm'], csvv['Vel_Viento_ms'], method='spearman'))
display(pg.corr(csvv['CO_ppm'], csvv['Vel_Viento_ms'], method='kendall'))

"""*   La primera variable que se analizó fue el monóxido con respecto a la velocidad del viento. Lo primero que podemos notar es que las r son negativas, por lo cual la relación entre esas dos variables es poco probable. Al mismo tiempo nuestro intervalo de confianza es negativo, por lo cual se puede afirmar que los datos se encuentran en el 5%, pero no dentro del 95%, esto significa que no es posible relacionar estas variables."""

#Analisis mas profundo sobre las correlaciones
display(pg.corr(csvv['CO_ppm'], csvv['NOX_ppb'], method='pearson'))
display(pg.corr(csvv['CO_ppm'], csvv['NOX_ppb'], method='spearman'))
display(pg.corr(csvv['CO_ppm'], csvv['NOX_ppb'], method='kendall'))

"""

*   Nos damos cuenta que el óxido de nitrógeno es una de las relaciones las cuales tiene mayor cobertura del intervalo de confianza, en promedio está en el 92% en relaciones a todos los datos, de la misma forma el valor de r está muy bien. Lo que nos quiere decir que hay una buena correlación entre esas variables. 
"""

#Analisis mas profundo sobre las correlaciones
display(pg.corr(csvv['CO_ppm'], csvv['NO_ppb'], method='pearson'))
display(pg.corr(csvv['CO_ppm'], csvv['NO_ppb'], method='spearman'))
display(pg.corr(csvv['CO_ppm'], csvv['NO_ppb'], method='kendall'))

""" 
*   Nos damos cuenta que el óxido nitrico es la segunda de las relaciones las cuales tiene mayor cobertura del intervalo de confianza, en promedio está en el 82,25% en relaciones a todos los datos, de la misma forma el valor de r está muy bien. Lo que nos quiere decir que hay una buena correlación entre esas variables. 



"""

#Exportamos la data final
Final_Data = csvv
Final_Data.to_csv('DataProcesada.csv')